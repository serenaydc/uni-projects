{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import randint, uniform, choice\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 8_000\n",
    "languages = [\"de\", \"en\", \"es\", \"fr\", \"nl\", \"pt\"]\n",
    "language_dict = {languages[i]: i for i in range(len(languages))}\n",
    "\n",
    "X_train, y_train = np.load(\"inputs_train_fp16.npy\"), np.load(\n",
    "    \"targets_train_int8.npy\"\n",
    ")\n",
    "X_test, y_test = np.load(\"inputs_test_fp16.npy\"), np.load(\n",
    "    \"targets_test_int8.npy\"\n",
    ")\n",
    "X_train, X_test = X_train.astype(np.float32), X_test.astype(np.float32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train = torch.from_numpy(X_train).to(device)\n",
    "X_test = torch.from_numpy(X_test).to(device)\n",
    "\n",
    "y_train = torch.from_numpy(y_train).to(device)\n",
    "y_test = torch.from_numpy(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization function\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(1, -1)\n",
    "        self.std = torch.tensor(std).view(1, -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "# Calculate mean and standard deviation of the training data\n",
    "mean = torch.tensor(X_train.mean(axis=0), dtype=torch.float32)\n",
    "std = torch.tensor(X_train.std(axis=0), dtype=torch.float32)\n",
    "\n",
    "# Define the normalization layer using the calculated mean and standard deviation\n",
    "normalization = Normalization(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LanguageClassifier model\n",
    "class LanguageClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob, normalization=normalization):\n",
    "        super(LanguageClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.normalization = normalization\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)  # Apply the normalization layer\n",
    "        x = x.unsqueeze(1)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the number of output classes and batch size\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "\n",
    "# Set the input size based on the actual size of the input features\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Define the random search parameters and ranges\n",
    "num_trials = 10\n",
    "hidden_size_range = (16, 32, 64, 128)\n",
    "num_layers_range = (1, 4)\n",
    "learning_rate_range = (1e-4, 1e-2)\n",
    "weight_decay_range = (1e-4, 1e-2)\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and accuracy\n",
    "best_hyperparameters = None\n",
    "best_accuracy = None\n",
    "best_loss = None\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "num_folds = 3\n",
    "num_epochs = 15\n",
    "dropout_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform random search\n",
    "for trial in range(num_trials):\n",
    "    # Generate random parameter values within the specified ranges\n",
    "    hidden_size = choice(hidden_size_range)\n",
    "    num_layers = randint(*num_layers_range)\n",
    "    learning_rate = uniform(*learning_rate_range)\n",
    "    weight_decay = uniform(*weight_decay_range)\n",
    "\n",
    "    # Print the current trial and corresponding hyperparameters\n",
    "    print(f\"Trial: {trial + 1}/{num_trials}\")\n",
    "    print(f\"Hyperparameters: input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, learning_rate={learning_rate}, weight_decay={weight_decay}\")\n",
    "\n",
    "    # Initialize lists to store fold accuracies and losses\n",
    "    fold_accuracies = []\n",
    "    fold_losses = []\n",
    "\n",
    "    # Perform stratified k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        # Create the model and other necessary components\n",
    "        model = LanguageClassifier(input_size, hidden_size, num_layers, num_classes, dropout_prob)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Convert data and labels to PyTorch tensors\n",
    "        train_data = torch.tensor(X_train[train_index], dtype=torch.float32).to(device)\n",
    "        train_labels = torch.tensor(y_train[train_index], dtype=torch.long).to(device)\n",
    "        val_data = torch.tensor(X_train[val_index], dtype=torch.float32).to(device)\n",
    "        val_labels = torch.tensor(y_train[val_index], dtype=torch.long).to(device)\n",
    "\n",
    "        # Convert data and labels to PyTorch Dataset\n",
    "        train_dataset = TensorDataset(train_data, train_labels)\n",
    "        val_dataset = TensorDataset(val_data, val_labels)\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on validation set after the last epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate accuracy and loss for the current fold\n",
    "        accuracy = correct / total\n",
    "        loss = avg_loss\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold_losses.append(loss)\n",
    "\n",
    "    # Calculate the average accuracy and loss across folds for the current trial\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "\n",
    "    # Print average accuracy and loss for the current trial\n",
    "    print(f\"Average accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "    # Update the best hyperparameters, accuracy, and loss if necessary\n",
    "    if best_accuracy is None or avg_accuracy > best_accuracy:\n",
    "        best_hyperparameters = {\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\" : weight_decay\n",
    "        }\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_loss = avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters, accuracy, and loss\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(\"Best Accuracy:\")\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best hyperparameters on the full training set and evaluate on the validation set\n",
    "best_model = LanguageClassifier(input_size, best_hyperparameters[\"hidden_size\"], best_hyperparameters[\"num_layers\"],\n",
    "                                num_classes, dropout_prob=0.5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "train_data = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "val_data = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "val_labels = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=best_hyperparameters[\"learning_rate\"], weight_decay=best_hyperparameters[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch_size\n",
    "        train_total += batch_size\n",
    "\n",
    "        # Calculate train accuracy\n",
    "        predicted_labels = outputs.argmax(dim=1)\n",
    "        train_correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "    # Calculate train accuracy\n",
    "    train_accuracy = train_correct / train_total * 100\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    best_model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        test_output = best_model(X_test)\n",
    "        _, predicted_labels = torch.max(test_output, dim=1)\n",
    "        test_correct += (predicted_labels == y_test).sum().item()\n",
    "        test_total += y_test.size(0)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = test_correct / test_total * 100\n",
    "\n",
    "    # Print train and test accuracy, and train loss\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss/train_total:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"          Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(best_model), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train.numpy())\n",
    "y_train_tensor = torch.from_numpy(y_train.numpy())\n",
    "\n",
    "model = torch.jit.load(\"model.pt\")\n",
    "model_outputs = model(X_train_tensor)\n",
    "targets = y_train_tensor\n",
    "\n",
    "# Apply PCA to the model outputs\n",
    "outputs_PCA = PCA(n_components=2).fit_transform(model_outputs.detach())\n",
    "\n",
    "# Create the scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(*outputs_PCA.T, c=targets, cmap=\"tab10\", alpha=0.3)\n",
    "\n",
    "# Create legend with unique colors from the scatter plot\n",
    "legend1 = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\n",
    "\n",
    "# Set legend labels as languages (replace with your own labels)\n",
    "languages = [\"de\", \"en\", \"es\", \"fr\", \"nl\", \"pt\"]\n",
    "for i, text in enumerate(legend1.get_texts()):\n",
    "    text.set_text(languages[i])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
