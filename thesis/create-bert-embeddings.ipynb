{"cells":[{"cell_type":"code","execution_count":64,"metadata":{"cell_id":"ed0735cd215a42229e112aee19556028","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1682691641467,"gather":{"logged":1681148737066},"source_hash":"3be2228"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import pandas as pd\n","import numpy as np\n","import re\n","import string\n","import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":65,"metadata":{"cell_id":"ad88d13fc92a45a5bcb33fbcc6c89bf1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":600,"execution_start":1682691650137,"gather":{"logged":1681148746068},"source_hash":"905a23ed"},"outputs":[],"source":["train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")"]},{"cell_type":"code","execution_count":66,"metadata":{"cell_id":"fc689f150ce740909df49dcf6407b01b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":171,"execution_start":1682691652117,"source_hash":"e79a6a0"},"outputs":[],"source":["# Combine the two features into one column\n","train[\"Combined\"] = train[\"Description\"] + \" \" + train[\"Title\"]\n","test[\"Combined\"] = test[\"Description\"] + \" \" + test[\"Title\"]"]},{"cell_type":"code","execution_count":67,"metadata":{"cell_id":"4c5f4be2e6af4c7796457d6dbdf9139d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":61,"execution_start":1682691653404,"source_hash":"a0fffb27"},"outputs":[],"source":["X_train = train[\"Combined\"]\n","X_test = test[\"Combined\"]\n","y_train = train[\"Class Index\"]\n","y_test = test[\"Class Index\"]"]},{"cell_type":"code","execution_count":68,"metadata":{"cell_id":"fc77ca62c85d4a6d9e8b3ae6e92d211c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1682691654725,"source_hash":"fd2a50b8"},"outputs":[],"source":["def clean_text(text):\n","    # Regular expression pattern to match HTML tags and HTML entities\n","    regex_html = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n","\n","    # Translation table to remove digits and punctuation marks\n","    remove_digits = str.maketrans('', '', string.digits + string.punctuation)\n","\n","    # Remove HTML tags and entities from the text\n","    text = re.sub(regex_html, '', text)\n","\n","    # Remove digits and punctuation marks from the text\n","    text = text.translate(remove_digits)\n","\n","    # Remove special characters, URLs, and usernames from the text\n","    text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text)\n","\n","    # Split the text into individual words, remove extra whitespace, and convert to lowercase\n","    text = ' '.join(text.split()).lower()\n","\n","    return text"]},{"cell_type":"code","execution_count":69,"metadata":{"cell_id":"2b65f696610a47489bcf7878ac397c37","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7423,"execution_start":1682691662163,"source_hash":"7485de48"},"outputs":[],"source":["# Apply data cleaning\n","X_train = X_train.apply(clean_text)\n","X_test = X_test.apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"87b6208584ae4ee3ada075873cf46711","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3752,"execution_start":1682691671356,"source_hash":"a806ee43"},"outputs":[],"source":["# Specify the name of the pre-trained BERT model to be used\n","model_name = \"bert-base-uncased\"\n","\n","# Create a tokenizer object using the specified BERT model\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Create a BERT model object using the specified BERT model\n","# Set the output_hidden_states parameter to True to retrieve hidden states from the model\n","model = AutoModel.from_pretrained(model_name, output_hidden_states=True)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# Calculate the maximum token length among all sequences in the training data\n","max_token_length = max([len(tokenizer.encode(seq)) for seq in X_train])"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"9c28aa7bc7ee48589ebda85598cc3f4e","deepnote_cell_type":"markdown"},"source":["## Training set"]},{"cell_type":"code","execution_count":89,"metadata":{"cell_id":"345ec4763f6b4095a4e8d9c8b0794723","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1682691720054,"gather":{"logged":1681148762153},"source_hash":"9d660573"},"outputs":[],"source":["# Define a function to obtain BERT embeddings\n","def get_bert_embeddings(text, max_length=max_token_length):\n","    # Tokenize input text\n","    tokens = tokenizer.encode(text, add_special_tokens=True, padding=\"max_length\", max_length=max_length)\n","    # Convert tokens to tensor\n","    tokens_tensor = torch.tensor(tokens).unsqueeze(0)\n","    # Obtain BERT embeddings\n","    with torch.no_grad():\n","        embeddings = model(tokens_tensor)[0].squeeze(0)\n","    # Return embeddings\n","    return embeddings.numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["CLS token"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the training set with using [CLS] token\n","\n","# Specify the batch size for processing the training data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the training data\n","num_batches = (X_train.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the training embeddings\n","training_embeddings = np.empty((X_train.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_train.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_train[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Store the [CLS] tokens in the batch embeddings array\n","        batch_embeddings[j, :] = embeddings[0]\n","    \n","    # Assign the batch embeddings to the corresponding indices in the training embeddings array\n","    training_embeddings[start_idx:end_idx, :] = batch_embeddings\n"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["np.save(\"bert_train_embeddings_cls\",training_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"10dd71f3c2d04615949e8f2d6ea25aa0","deepnote_cell_type":"markdown"},"source":["Mean - pooling"]},{"cell_type":"code","execution_count":108,"metadata":{"cell_id":"e24eebae67bb4e079ba08c613955c030","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_start":1682691756743,"source_hash":"e454bed8"},"outputs":[],"source":["# Obtain BERT embeddings for the training set with mean pooling\n","\n","# Specify the batch size for processing the training data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the training data\n","num_batches = (X_train.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the training embeddings\n","training_embeddings = np.empty((X_train.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_train.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_train[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Compute the sentence-level embedding by taking the mean of the token embeddings\n","        sentence_embedding = np.mean(embeddings, 0)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the training embeddings array\n","    training_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":109,"metadata":{"cell_id":"af68779272a74fccb678d21bf57f9eea","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"f8c9cd51"},"outputs":[],"source":["np.save(\"bert_train_embeddings_mean\",training_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d7f20368024149ca97a3844eedac1055","deepnote_cell_type":"markdown"},"source":["Max - pooling"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"786f3a6fa3f14f45a96b89862a9251a8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"245f18c3"},"outputs":[],"source":["# Obtain BERT embeddings for the training set with max pooling\n","\n","# Specify the batch size for processing the training data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the training data\n","num_batches = (X_train.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the training embeddings\n","training_embeddings = np.empty((X_train.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_train.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_train[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Compute the sentence-level embedding by taking the maximum value along the axis 0 (across tokens)\n","        sentence_embedding = np.max(embeddings, axis=0)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the training embeddings array\n","    training_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"40f1c372bf0948c99b37cc801048d090","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"f6fcdd4a"},"outputs":[],"source":["np.save(\"bert_train_embeddings_max\",training_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a8b69b43e4e94e53b83b0eb695ee0d19","deepnote_cell_type":"markdown"},"source":["Attention - pooling"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the training set with attention pooling\n","\n","# Define an attention layer\n","attention_layer = nn.Linear(model.config.hidden_size, 1)\n","\n","def get_attention_pooled_embedding(text):\n","    tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_token_length,\n","                                    padding='max_length', return_tensors='pt')\n","    tokens_tensor = tokens['input_ids']\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_mask=(tokens_tensor > 0))[0]\n","        hidden_states = outputs\n","\n","    # Compute attention weights\n","    attention_weights = torch.softmax(attention_layer(hidden_states), dim=1)\n","\n","    # Apply attention pooling\n","    sentence_embedding = torch.sum(hidden_states * attention_weights, dim=1).squeeze()\n","\n","    return sentence_embedding.detach().numpy()\n","\n","batch_size = 512\n","num_batches = (X_train.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the training embeddings\n","training_embeddings = np.empty((X_train.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_train.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_train[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the sentence-level embedding using the get_attention_pooled_embedding function\n","        sentence_embedding = get_attention_pooled_embedding(text)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the training embeddings array\n","    training_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":115,"metadata":{"cell_id":"c2f6fe38c3e544d09c515db98497f64c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"4c45593b"},"outputs":[],"source":["np.save(\"bert_train_embeddings_attention\",training_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Mixed - pooling"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the training set with mixed pooling\n","def get_mixed_pooled_embedding(text):\n","    tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_token_length, truncation=True,\n","                                    padding='max_length', return_tensors='pt')\n","    tokens_tensor = tokens['input_ids']\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_mask=(tokens_tensor > 0))[0]\n","        hidden_states = outputs\n","    \n","    # Mixed pooling\n","    layer1_embedding = torch.mean(hidden_states[:, :6, :], dim=1) # Average pooling on first 6 layers\n","    layer2_embedding = torch.max(hidden_states[:, 6:, :], dim=1).values # Max pooling on remaining layers\n","    \n","    # Concatenate layer embeddings\n","    sentence_embedding = torch.cat((layer1_embedding, layer2_embedding), dim=1) \n","    return sentence_embedding\n","\n","batch_size = 512\n","num_batches = (X_train.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","training_embeddings = np.empty((X_train.shape[0], 1536)) # Updated embedding size due to mixed pooling\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_train.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_train[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 1536))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the sentence-level embedding using the get_mixed_pooled_embedding function\n","        sentence_embedding = get_mixed_pooled_embedding(text)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the training embeddings array\n","    training_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["np.save(\"bert_train_embeddings_mixed\",training_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c9477ca46b7c49be9214a1a2269c94b4","deepnote_cell_type":"markdown"},"source":["## Test set"]},{"cell_type":"code","execution_count":119,"metadata":{"cell_id":"05859c4d0aa04141aa2b1d1105c209ca","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"e8464c7a"},"outputs":[],"source":["# Define a function to obtain BERT embeddings\n","def get_bert_embeddings(text, max_length=max_token_length):\n","    # Tokenize input text\n","    tokens = tokenizer.encode(text, add_special_tokens=True, padding=\"max_length\", max_length=max_length)\n","    # Convert tokens to tensor\n","    tokens_tensor = torch.tensor(tokens).unsqueeze(0)\n","    # Obtain BERT embeddings\n","    with torch.no_grad():\n","        embeddings = model(tokens_tensor)[0].squeeze(0)\n","    # Return embeddings\n","    return embeddings.numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["CLS Token"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the test set with using [CLS] token\n","\n","# Specify the batch size for processing the test data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the test data\n","num_batches = (X_test.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the test embeddings\n","test_embeddings = np.empty((X_test.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_test.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_test[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Store the [CLS] tokens in the batch embeddings array\n","        batch_embeddings[j, :] = embeddings[0]\n","    \n","    # Assign the batch embeddings to the corresponding indices in the test embeddings array\n","    test_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":["np.save(\"bert_test_embeddings_cls\",test_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"69e4c5a2b32b4582898df146a629a654","deepnote_cell_type":"markdown"},"source":["Mean - pooling"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the test set with mean pooling\n","\n","# Specify the batch size for processing the test data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the test data\n","num_batches = (X_test.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the test embeddings\n","test_embeddings = np.empty((X_test.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_test.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_test[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Compute the sentence-level embedding by taking the mean of the token embeddings\n","        sentence_embedding = np.mean(embeddings, 0)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the test embeddings array\n","    test_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":124,"metadata":{"cell_id":"5cf7ba9e25ae4ab0b10668985fb85b25","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"464dd5f1"},"outputs":[],"source":["np.save(\"bert_test_embeddings_mean\",test_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"1bc63a2ac6514a14a8f3f990feca7a6b","deepnote_cell_type":"markdown"},"source":["Max - pooling"]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"082b3c2854b14bc6a170e847365b91bc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"74396c53"},"outputs":[],"source":["# Obtain BERT embeddings for the test set with max pooling\n","\n","# Specify the batch size for processing the test data\n","batch_size = 512\n","\n","# Calculate the number of batches required to process the test data\n","num_batches = (X_test.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the test embeddings\n","test_embeddings = np.empty((X_test.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_test.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_test[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the BERT embeddings for the current text\n","        embeddings = get_bert_embeddings(text)\n","        \n","        # Compute the sentence-level embedding by taking the maximum value along the axis 0 (across tokens)\n","        sentence_embedding = np.max(embeddings, axis=0)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the test embeddings array\n","    test_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"b60003f7b288461f8632af3fc9fa3f82","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"f48d1cb3"},"outputs":[],"source":["np.save(\"bert_test_embeddings_max\",test_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"294035a485fa41b4b8e891bc5bb7ef89","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["Attention - pooling"]},{"cell_type":"code","execution_count":126,"metadata":{"cell_id":"6574cdc6f49e425392b864ec351e364f","deepnote_cell_type":"code"},"outputs":[],"source":["# Obtain BERT embeddings for the test set with attention pooling\n","\n","# Define an attention layer\n","attention_layer = nn.Linear(model.config.hidden_size, 1)\n","\n","def get_attention_pooled_embedding(text):\n","    tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_token_length,\n","                                    padding='max_length', return_tensors='pt')\n","    tokens_tensor = tokens['input_ids']\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_mask=(tokens_tensor > 0))[0]\n","        hidden_states = outputs\n","\n","    # Compute attention weights\n","    attention_weights = torch.softmax(attention_layer(hidden_states), dim=1)\n","\n","    # Apply attention pooling\n","    sentence_embedding = torch.sum(hidden_states * attention_weights, dim=1).squeeze()\n","\n","    return sentence_embedding.detach().numpy()\n","\n","batch_size = 512\n","num_batches = (X_test.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","# Create an empty array to store the test embeddings\n","test_embeddings = np.empty((X_test.shape[0], 768))\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_test.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_test[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 768))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the sentence-level embedding using the get_attention_pooled_embedding function\n","        sentence_embedding = get_attention_pooled_embedding(text)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the test embeddings array\n","    test_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":127,"metadata":{"cell_id":"2d0704c21d0541348ba27fa7b4f0de3a","deepnote_cell_type":"code"},"outputs":[],"source":["np.save(\"bert_test_embeddings_attention\",test_embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Mixed - pooling"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["# Obtain BERT embeddings for the test set with mixed pooling\n","def get_mixed_pooled_embedding(text):\n","    tokens = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_token_length, truncation=True,\n","                                    padding='max_length', return_tensors='pt')\n","    tokens_tensor = tokens['input_ids']\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_mask=(tokens_tensor > 0))[0]\n","        hidden_states = outputs\n","    \n","    # Mixed pooling\n","    layer1_embedding = torch.mean(hidden_states[:, :6, :], dim=1) # Average pooling on first 6 layers\n","    layer2_embedding = torch.max(hidden_states[:, 6:, :], dim=1).values # Max pooling on remaining layers\n","    \n","    # Concatenate layer embeddings\n","    sentence_embedding = torch.cat((layer1_embedding, layer2_embedding), dim=1) \n","    return sentence_embedding\n","\n","batch_size = 512\n","num_batches = (X_test.shape[0] + batch_size - 1) // batch_size  # Round up to the nearest integer\n","\n","test_embeddings = np.empty((X_test.shape[0], 1536)) # Updated embedding size due to mixed pooling\n","\n","# Iterate over the batches\n","for i in range(num_batches):\n","    # Determine the start and end indices for the current batch\n","    start_idx = i * batch_size\n","    end_idx = min((i+1) * batch_size, X_test.shape[0])\n","    \n","    # Extract the texts for the current batch\n","    batch_texts = X_test[start_idx:end_idx]\n","    \n","    # Create an empty array to store the embeddings for the current batch\n","    batch_embeddings = np.empty((len(batch_texts), 1536))\n","    \n","    # Iterate over the texts in the current batch\n","    for j, text in enumerate(batch_texts):\n","        # Obtain the sentence-level embedding using the get_mixed_pooled_embedding function\n","        sentence_embedding = get_mixed_pooled_embedding(text)\n","        \n","        # Store the sentence-level embedding in the batch embeddings array\n","        batch_embeddings[j, :] = sentence_embedding\n","    \n","    # Assign the batch embeddings to the corresponding indices in the test embeddings array\n","    test_embeddings[start_idx:end_idx, :] = batch_embeddings"]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":["np.save(\"bert_test_embeddings_mixed\",test_embeddings)"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"584ad7a8ca42400bad1298daf8b28bb2","kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"microsoft":{"ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ddaa0467fde5894aa98f0ffdc083817e4a539d0b6eeb0d6547bba99722a22718"}}},"nbformat":4,"nbformat_minor":0}
